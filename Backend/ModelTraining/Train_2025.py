#Train_2025.py
import pandas as pd
import numpy as np
import joblib
import json
import os
from datetime import datetime
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_curve
from xgboost import XGBClassifier
import optuna
import warnings
warnings.filterwarnings('ignore')

print("\n" + "="*70)
print(" üöÄ TRAINING MODEL 2025 - WITH DETAILED REPORTING")
print("="*70)

# Create reports directory
os.makedirs("Models/2025/reports", exist_ok=True)

# ---------------------------------------------------
# STEP 1: Load data
# ---------------------------------------------------
print("\nüì• Loading data...")
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.join(script_dir, '..', '..')
features_path = os.path.join(project_root, "Models/2025/features_2025.pkl")
labels_path = os.path.join(project_root, "Models/2025/labels_2025.pkl")

X = joblib.load(features_path)
y = joblib.load(labels_path)
print(f"‚úÖ Features loaded: {X.shape}")
print(f"‚úÖ Labels loaded: {len(y)}")

# ---------------------------------------------------
# STEP 2: Validate labels
# ---------------------------------------------------
print("\nüîç Validating labels...")
y = pd.Series(y)

if y.isnull().any():
    missing_count = y.isnull().sum()
    print(f"‚ö†Ô∏è  WARNING: {missing_count} missing labels detected!")
    valid_indices = ~y.isnull()
    X = X[valid_indices]
    y = y[valid_indices]
    print(f"‚úÖ After cleaning: {len(y)} samples remain")

if y.dtype == 'object':
    label_mapping = {'Legitimate': 0, 'Phishing': 1}
    y = y.map(label_mapping)
    if y.isnull().any():
        print("‚ùå ERROR: Some labels couldn't be mapped!")
        raise ValueError("Invalid label format")
else:
    if not set(y.unique()).issubset({0, 1}):
        print("‚ùå ERROR: Numeric labels must be 0 or 1")
        raise ValueError(f"Invalid numeric labels: {y.unique()}")

y = y.values

print(f"‚úÖ Label distribution:")
unique, counts = np.unique(y, return_counts=True)
label_distribution = {}
for label, count in zip(unique, counts):
    label_name = 'Legitimate' if label == 0 else 'Phishing'
    label_distribution[label_name] = int(count)
    print(f"   {label_name}: {count} ({count/len(y)*100:.1f}%)")

# ---------------------------------------------------
# STEP 3: Train-test split
# ---------------------------------------------------
print("\nüìä Splitting data...")
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
print(f"‚úÖ Training set: {X_train.shape[0]} samples")
print(f"‚úÖ Test set: {X_test.shape[0]} samples")

# ---------------------------------------------------
# STEP 4: Hyperparameter optimization
# ---------------------------------------------------
print("\nüîç Starting hyperparameter optimization...")

def objective(trial):
    # ============================================
    # ANTI-OVERFITTING PARAMETERS
    # ============================================
    params = {
        # 1. SHALLOW TREES - Prevent complex memorization
        "max_depth": trial.suggest_int("max_depth", 2, 4),
        
        # 2. STRONG L2 REGULARIZATION - Force simple patterns
        "reg_lambda": trial.suggest_float("reg_lambda", 5.0, 20.0),
        
        # 3. STRONG L1 REGULARIZATION - Feature selection
        "reg_alpha": trial.suggest_float("reg_alpha", 2.0, 10.0),
        
        # 4. HIGH MINIMUM SAMPLES - Prevent splitting on noise
        "min_child_weight": trial.suggest_int("min_child_weight", 5, 20),
        
        # 5. AGGRESSIVE SUBSAMPLING - Train on different data each iteration
        "subsample": trial.suggest_float("subsample", 0.5, 0.7),
        
        # 6. FEATURE DROPOUT - Prevent reliance on specific features
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.4, 0.6),
        "colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.4, 0.6),
        "colsample_bynode": trial.suggest_float("colsample_bynode", 0.4, 0.6),
        
        # 7. CONSERVATIVE LEARNING - Slow, gradual pattern learning
        "learning_rate": trial.suggest_float("learning_rate", 0.005, 0.05, log=True),
        
        # 8. FEWER TREES - Prevent ensemble memorization
        "n_estimators": trial.suggest_int("n_estimators", 50, 200),
        
        # 9. STRONG PRUNING - Remove complex branches
        "gamma": trial.suggest_float("gamma", 0.5, 2.0),
        
        # Standard
        "objective": "binary:logistic",
        "eval_metric": "logloss",
        "random_state": 42,
        "verbosity": 0
    }
    model = XGBClassifier(**params)
    # Increased CV folds for better generalization assessment
    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=-1)
    return cv_scores.mean()


study = optuna.create_study(
    direction="maximize",
    study_name="xgboost_2025_pattern_learning",
    pruner=optuna.pruners.MedianPruner(
        n_startup_trials=5, 
        n_warmup_steps=3,    
        interval_steps=1
    )
)

study.optimize(objective, n_trials=25, show_progress_bar=True, n_jobs=1)

print(f"\n‚úÖ Best CV Accuracy: {study.best_value:.4f}")

# ---------------------------------------------------
# STEP 5: Train final model
# ---------------------------------------------------
print("\nü§ñ Training final model with early stopping...")
final_model = XGBClassifier(**study.best_params, eval_metric='logloss')
# Add early stopping to prevent overfitting during training
final_model.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    verbose=False
)
print("‚úÖ Model training complete!")
if hasattr(final_model, 'best_iteration'):
    print(f"   Best iteration: {final_model.best_iteration}")

# ---------------------------------------------------
# STEP 6: Evaluate
# ---------------------------------------------------
print("\nüìä Evaluating model...")

# Cross-validation
# Use same 10-fold CV for consistency
cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
cv_scores = cross_val_score(final_model, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=-1)

# Test predictions
y_pred_test = final_model.predict(X_test)
y_pred_proba_test = final_model.predict_proba(X_test)[:, 1]
test_accuracy = accuracy_score(y_test, y_pred_test)

# Train predictions
y_pred_train = final_model.predict(X_train)
train_accuracy = accuracy_score(y_train, y_pred_train)

# Confusion matrix
cm = confusion_matrix(y_test, y_pred_test)

print(f"‚úÖ Test Accuracy: {test_accuracy:.4f}")
print(f"‚úÖ Train Accuracy: {train_accuracy:.4f}")

# ============================================================
# NEW: STEP 6.5 - THRESHOLD OPTIMIZATION
# ============================================================
print("\n" + "="*70)
print(" üéØ OPTIMIZING DECISION THRESHOLD (FALSE POSITIVE REDUCTION)")
print("="*70)

# Get predicted probabilities
y_proba = final_model.predict_proba(X_test)[:, 1]

# Calculate precision-recall curve
precisions, recalls, thresholds = precision_recall_curve(y_test, y_proba)

# Method 1: F1-optimized threshold (balanced)
f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)
best_f1_idx = np.argmax(f1_scores)
f1_threshold = thresholds[best_f1_idx] if best_f1_idx < len(thresholds) else 0.5

# Method 2: High-precision threshold (minimize false positives)
# Find threshold where precision >= 0.95
high_precision_idx = np.where(precisions >= 0.95)[0]
if len(high_precision_idx) > 0:
    high_precision_threshold = thresholds[high_precision_idx[0]]
else:
    high_precision_threshold = 0.65  # Conservative fallback

# Method 3: Balanced threshold (precision ‚âà recall)
precision_recall_diff = np.abs(precisions - recalls)
balanced_idx = np.argmin(precision_recall_diff)
balanced_threshold = thresholds[balanced_idx] if balanced_idx < len(thresholds) else 0.5

# ============================================================
# Choose optimal threshold: Use high-precision for FP reduction
# ============================================================
optimal_threshold = high_precision_threshold

print(f"\nüìä Threshold Analysis:")
print(f"   Default threshold:        0.5000")
print(f"   F1-optimized threshold:   {f1_threshold:.4f}")
print(f"   Balanced threshold:       {balanced_threshold:.4f}")
print(f"   High-precision threshold: {high_precision_threshold:.4f}")
print(f"\n   ‚úÖ Selected: {optimal_threshold:.4f} (high-precision)")

# Make predictions with optimal threshold
y_pred_optimal = (y_proba >= optimal_threshold).astype(int)
test_accuracy_optimal = accuracy_score(y_test, y_pred_optimal)

# Confusion matrices
cm_default = confusion_matrix(y_test, y_pred_test)
cm_optimal = confusion_matrix(y_test, y_pred_optimal)

# Calculate metrics
fpr_default = cm_default[0][1] / (cm_default[0][0] + cm_default[0][1]) if (cm_default[0][0] + cm_default[0][1]) > 0 else 0
fpr_optimal = cm_optimal[0][1] / (cm_optimal[0][0] + cm_optimal[0][1]) if (cm_optimal[0][0] + cm_optimal[0][1]) > 0 else 0

fnr_default = cm_default[1][0] / (cm_default[1][0] + cm_default[1][1]) if (cm_default[1][0] + cm_default[1][1]) > 0 else 0
fnr_optimal = cm_optimal[1][0] / (cm_optimal[1][0] + cm_optimal[1][1]) if (cm_optimal[1][0] + cm_optimal[1][1]) > 0 else 0

precision_default = cm_default[1][1] / (cm_default[1][1] + cm_default[0][1]) if (cm_default[1][1] + cm_default[0][1]) > 0 else 0
precision_optimal = cm_optimal[1][1] / (cm_optimal[1][1] + cm_optimal[0][1]) if (cm_optimal[1][1] + cm_optimal[0][1]) > 0 else 0

recall_default = cm_default[1][1] / (cm_default[1][1] + cm_default[1][0]) if (cm_default[1][1] + cm_default[1][0]) > 0 else 0
recall_optimal = cm_optimal[1][1] / (cm_optimal[1][1] + cm_optimal[1][0]) if (cm_optimal[1][1] + cm_optimal[1][0]) > 0 else 0

print(f"\nüìä Performance Comparison:")
print(f"{'Metric':<30} {'Default (0.5)':<15} {'Optimal ({:.2f})'.format(optimal_threshold):<15} {'Change':<10}")
print("-"*70)
print(f"{'Accuracy':<30} {test_accuracy:.4f} ({test_accuracy*100:.1f}%)  {test_accuracy_optimal:.4f} ({test_accuracy_optimal*100:.1f}%)  {(test_accuracy_optimal-test_accuracy)*100:+.1f}%")
print(f"{'False Positive Rate':<30} {fpr_default:.4f} ({fpr_default*100:.1f}%)  {fpr_optimal:.4f} ({fpr_optimal*100:.1f}%)  {(fpr_optimal-fpr_default)*100:+.1f}%")
print(f"{'False Negative Rate':<30} {fnr_default:.4f} ({fnr_default*100:.1f}%)  {fnr_optimal:.4f} ({fnr_optimal*100:.1f}%)  {(fnr_optimal-fnr_default)*100:+.1f}%")
print(f"{'Precision (Phishing)':<30} {precision_default:.4f} ({precision_default*100:.1f}%)  {precision_optimal:.4f} ({precision_optimal*100:.1f}%)  {(precision_optimal-precision_default)*100:+.1f}%")
print(f"{'Recall (Phishing)':<30} {recall_default:.4f} ({recall_default*100:.1f}%)  {recall_optimal:.4f} ({recall_optimal*100:.1f}%)  {(recall_optimal-recall_default)*100:+.1f}%")

print(f"\nüí° Key Improvement:")
if fpr_optimal < fpr_default:
    fpr_reduction = (fpr_default - fpr_optimal) * 100
    print(f"   ‚úÖ False positive rate reduced by {fpr_reduction:.1f} percentage points")
    print(f"   This means fewer legitimate sites wrongly flagged as phishing!")
else:
    print(f"   ‚ö†Ô∏è  No improvement in false positive rate")

# ---------------------------------------------------
# STEP 7: Generate Detailed Reports
# ---------------------------------------------------
print("\n" + "="*70)
print(" üìù GENERATING DETAILED REPORTS")
print("="*70)

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# ========================================================
# REPORT 1: Training Summary (JSON)
# ========================================================
training_summary = {
    "training_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    "model_type": "XGBoost Classifier",
    "label_encoding": "0=Legitimate, 1=Phishing",
    "dataset": {
        "total_samples": len(X),
        "training_samples": len(X_train),
        "test_samples": len(X_test),
        "num_features": len(X.columns),
        "label_distribution": label_distribution
    },
    "hyperparameters": study.best_params,
    "performance": {
        "test_accuracy": float(test_accuracy),
        "train_accuracy": float(train_accuracy),
        "cv_accuracy_mean": float(cv_scores.mean()),
        "cv_accuracy_std": float(cv_scores.std()),
        "overfitting_gap": float(train_accuracy - test_accuracy)
    },
    "threshold_optimization": {
        "default_threshold": 0.5,
        "optimal_threshold": float(optimal_threshold),
        "f1_threshold": float(f1_threshold),
        "balanced_threshold": float(balanced_threshold),
        "selected_method": "high_precision"
    },
    "performance_with_optimal_threshold": {
        "test_accuracy": float(test_accuracy_optimal),
        "false_positive_rate": float(fpr_optimal),
        "false_negative_rate": float(fnr_optimal),
        "precision": float(precision_optimal),
        "recall": float(recall_optimal),
        "fpr_reduction": float(fpr_default - fpr_optimal)
    },
    "confusion_matrix_default": {
        "true_negatives": int(cm_default[0][0]),
        "false_positives": int(cm_default[0][1]),
        "false_negatives": int(cm_default[1][0]),
        "true_positives": int(cm_default[1][1])
    },
    "confusion_matrix_optimal": {
        "true_negatives": int(cm_optimal[0][0]),
        "false_positives": int(cm_optimal[0][1]),
        "false_negatives": int(cm_optimal[1][0]),
        "true_positives": int(cm_optimal[1][1])
    }
}

json_path = "Models/2025/reports/training_summary_latest2025.json"
with open(json_path, 'w') as f:
    json.dump(training_summary, f, indent=4)
print(f"‚úÖ Saved: training_summary_latest2025.json")

# ========================================================
# REPORT 2: Feature Importance (CSV & TXT)
# ========================================================
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': final_model.feature_importances_
}).sort_values('importance', ascending=False)

csv_path = "Models/2025/reports/feature_importance_latest2025.csv"
feature_importance.to_csv(csv_path, index=False)
print(f"‚úÖ Saved: feature_importance_latest2025.csv")

# ========================================================
# REPORT 3: Human-Readable Report (TXT)
# ========================================================
report_lines = []
report_lines.append("="*80)
report_lines.append(" MODEL 2025 - TRAINING REPORT (ENHANCED)")
report_lines.append("="*80)
report_lines.append(f"\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
report_lines.append(f"\n{'='*80}")
report_lines.append("\nüìä DATASET INFORMATION")
report_lines.append("-"*80)
report_lines.append(f"Total Samples:          {len(X):,}")
report_lines.append(f"Training Samples:       {len(X_train):,}")
report_lines.append(f"Test Samples:           {len(X_test):,}")
report_lines.append(f"Number of Features:     {len(X.columns)}")
report_lines.append(f"\nLabel Distribution:")
report_lines.append(f"  Legitimate:           {label_distribution.get('Legitimate', 0):,} ({label_distribution.get('Legitimate', 0)/len(y)*100:.1f}%)")
report_lines.append(f"  Phishing:             {label_distribution.get('Phishing', 0):,} ({label_distribution.get('Phishing', 0)/len(y)*100:.1f}%)")

report_lines.append(f"\n{'='*80}")
report_lines.append("\nüéØ MODEL PERFORMANCE (DEFAULT THRESHOLD = 0.5)")
report_lines.append("-"*80)
report_lines.append(f"Test Accuracy:          {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")
report_lines.append(f"Train Accuracy:         {train_accuracy:.4f} ({train_accuracy*100:.2f}%)")
report_lines.append(f"CV Accuracy:            {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")
report_lines.append(f"Overfitting Gap:        {train_accuracy - test_accuracy:.4f}")

if train_accuracy - test_accuracy < 0.02:
    report_lines.append("  ‚úÖ Excellent - Minimal overfitting")
elif train_accuracy - test_accuracy < 0.05:
    report_lines.append("  ‚úÖ Good - Slight overfitting (acceptable)")
else:
    report_lines.append("  ‚ö†Ô∏è  Warning - Moderate to high overfitting detected")

report_lines.append(f"\n{'='*80}")
report_lines.append("\nüéØ THRESHOLD OPTIMIZATION (FALSE POSITIVE REDUCTION)")
report_lines.append("-"*80)
report_lines.append(f"Default Threshold:      0.5000")
report_lines.append(f"Optimal Threshold:      {optimal_threshold:.4f} (high-precision)")
report_lines.append(f"F1-Optimized:           {f1_threshold:.4f}")
report_lines.append(f"Balanced:               {balanced_threshold:.4f}")
report_lines.append(f"\nPerformance with Optimal Threshold:")
report_lines.append(f"  Test Accuracy:        {test_accuracy_optimal:.4f} ({test_accuracy_optimal*100:.2f}%)")
report_lines.append(f"  False Positive Rate:  {fpr_optimal:.4f} ({fpr_optimal*100:.2f}%)")
report_lines.append(f"  False Negative Rate:  {fnr_optimal:.4f} ({fnr_optimal*100:.2f}%)")
report_lines.append(f"  Precision:            {precision_optimal:.4f} ({precision_optimal*100:.2f}%)")
report_lines.append(f"  Recall:               {recall_optimal:.4f} ({recall_optimal*100:.2f}%)")
report_lines.append(f"\nImprovement:")
report_lines.append(f"  FPR Reduction:        {(fpr_default - fpr_optimal)*100:.2f} percentage points")

report_lines.append(f"\n{'='*80}")
report_lines.append("\nüìã CONFUSION MATRIX (DEFAULT THRESHOLD)")
report_lines.append("-"*80)
report_lines.append(f"True Negatives (Legit ‚Üí Legit):      {cm_default[0][0]:,}")
report_lines.append(f"False Positives (Legit ‚Üí Phishing):  {cm_default[0][1]:,}")
report_lines.append(f"False Negatives (Phishing ‚Üí Legit):  {cm_default[1][0]:,}")
report_lines.append(f"True Positives (Phishing ‚Üí Phishing): {cm_default[1][1]:,}")
report_lines.append(f"\nFalse Positive Rate:    {fpr_default:.4f} ({fpr_default*100:.2f}%)")
report_lines.append(f"False Negative Rate:    {fnr_default:.4f} ({fnr_default*100:.2f}%)")

report_lines.append(f"\n{'='*80}")
report_lines.append("\nüìã CONFUSION MATRIX (OPTIMAL THRESHOLD = {:.4f})".format(optimal_threshold))
report_lines.append("-"*80)
report_lines.append(f"True Negatives (Legit ‚Üí Legit):      {cm_optimal[0][0]:,}")
report_lines.append(f"False Positives (Legit ‚Üí Phishing):  {cm_optimal[0][1]:,}")
report_lines.append(f"False Negatives (Phishing ‚Üí Legit):  {cm_optimal[1][0]:,}")
report_lines.append(f"True Positives (Phishing ‚Üí Phishing): {cm_optimal[1][1]:,}")
report_lines.append(f"\nFalse Positive Rate:    {fpr_optimal:.4f} ({fpr_optimal*100:.2f}%)")
report_lines.append(f"False Negative Rate:    {fnr_optimal:.4f} ({fnr_optimal*100:.2f}%)")

report_lines.append(f"\n{'='*80}")
report_lines.append("\nüèÜ TOP 20 MOST IMPORTANT FEATURES")
report_lines.append("-"*80)
for idx, row in feature_importance.head(20).iterrows():
    report_lines.append(f"{row['feature']:<30} {row['importance']:.6f}")

report_lines.append(f"\n{'='*80}")
report_lines.append("\n‚öôÔ∏è  HYPERPARAMETERS")
report_lines.append("-"*80)
for param, value in study.best_params.items():
    report_lines.append(f"{param:<25} {value}")

report_lines.append(f"\n{'='*80}")
report_lines.append("\nüìå LABEL ENCODING")
report_lines.append("-"*80)
report_lines.append("Class 0 = Legitimate (Safe websites)")
report_lines.append("Class 1 = Phishing (Malicious websites)")
report_lines.append("\nWhen predict_proba() returns [0.9, 0.1]:")
report_lines.append("  - 90% probability of Legitimate (index 0)")
report_lines.append("  - 10% probability of Phishing (index 1)")

report_lines.append(f"\n{'='*80}")
report_lines.append("\nüí° USAGE NOTES")
report_lines.append("-"*80)
report_lines.append(f"To use optimal threshold in production:")
report_lines.append(f"  1. Get probabilities: proba = model.predict_proba(X)[:, 1]")
report_lines.append(f"  2. Apply threshold: predictions = (proba >= {optimal_threshold:.4f}).astype(int)")
report_lines.append(f"  3. This reduces false positives while maintaining phishing detection!")

report_lines.append(f"\n{'='*80}")
report_lines.append("\n‚úÖ TRAINING COMPLETED SUCCESSFULLY")
report_lines.append("="*80)

txt_path = "Models/2025/reports/training_report_latest2025.txt"
with open(txt_path, 'w', encoding='utf-8') as f:
    f.write('\n'.join(report_lines))
print(f"‚úÖ Saved: training_report_latest2025.txt")

# ========================================================
# REPORT 4: Test Predictions Sample (CSV)
# ========================================================
test_results = pd.DataFrame({
    'actual_label': y_test,
    'predicted_label_default': y_pred_test,
    'predicted_label_optimal': y_pred_optimal,
    'prob_legitimate': 1 - y_pred_proba_test,
    'prob_phishing': y_pred_proba_test,
    'correct_default': y_test == y_pred_test,
    'correct_optimal': y_test == y_pred_optimal
})

# Add human-readable labels
test_results['actual_class'] = test_results['actual_label'].map({0: 'Legitimate', 1: 'Phishing'})
test_results['predicted_class_default'] = test_results['predicted_label_default'].map({0: 'Legitimate', 1: 'Phishing'})
test_results['predicted_class_optimal'] = test_results['predicted_label_optimal'].map({0: 'Legitimate', 1: 'Phishing'})

csv_path = "Models/2025/reports/test_predictions_latest2025.csv"
test_results.to_csv(csv_path, index=False)
print(f"‚úÖ Saved: test_predictions_latest2025.csv")

# Show fixed false positives
print("\nüîç False Positives Fixed by Optimal Threshold:")
fp_fixed = test_results[
    (test_results['actual_label'] == 0) & 
    (test_results['predicted_label_default'] == 1) & 
    (test_results['predicted_label_optimal'] == 0)
]
if len(fp_fixed) > 0:
    print(f"   ‚úÖ Fixed {len(fp_fixed)} false positives!")
    print(f"   Sample:")
    print(fp_fixed[['actual_class', 'predicted_class_default', 'predicted_class_optimal', 'prob_phishing']].head(5).to_string(index=False))
else:
    print("   No false positives fixed (threshold may need adjustment)")

# Show sample misclassifications (with optimal threshold)
print("\nüîç Remaining Misclassifications (Optimal Threshold):")
misclassified = test_results[~test_results['correct_optimal']].head(10)
if len(misclassified) > 0:
    print(misclassified[['actual_class', 'predicted_class_optimal', 'prob_phishing']].to_string(index=False))
else:
    print("   Perfect classification!")

# ============================================================
# NEW: REPORT 5 - VALIDATION ON COMPLEX URLS
# ============================================================
print("\n" + "="*70)
print(" üî¨ VALIDATION: TESTING ON COMPLEX LEGITIMATE URLS")
print("="*70)

# Import feature extractor
import sys
sys.path.append(os.path.join(project_root, 'Data_Preprocessing'))
from feature_extractor import URLFeatureExtractor

complex_test_urls = [
    "https://www.britannica.com/biography/Che-Guevara",
    "https://plato.stanford.edu/entries/ethics-ai/",
    "https://www.khanacademy.org/math/algebra/x2f8bb11595b61c86:systems-of-equations",
    "https://scholar.google.com/scholar?hl=en&q=phishing+detection",
    "https://docs.google.com/document/d/1A9f8KJ9P2wQxU4Y/edit",
    "https://learn.microsoft.com/en-us/azure/active-directory/develop/v2-oauth2-auth-code-flow",
    "https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise",
    "https://github.com/openai/gpt-4/blob/main/system_card.md",
    "https://auth0.com/docs/flows/authorization-code-flow",
    "https://www.youtube.com/watch?v=dQw4w9WgXcQ",
    "https://medium.com/@user/how-machine-learning-detects-phishing-8f92a9c12",
    "https://www.reddit.com/r/netsec/comments/15f9k2m/phishing_detection_models/",
    "https://vimeo.com/76979871",
    "https://login.microsoftonline.com/common/oauth2/v2.0/authorize",
    "https://accounts.google.com/o/oauth2/v2/auth",
    "https://www.nsf.gov/awardsearch/showAward?AWD_ID=2033955",
    "https://www.apu.edu.my/research/publications/2023/ai-security-models",
    "https://www.baseball-almanac.com/teamstats/roster.php?y=1988&t=MON",
    "https://www.americansongwriter.com/2010/04/disney-music-group-folds-lyric-street-records/",
]

print(f"\nüìã Testing {len(complex_test_urls)} complex legitimate URLs...")

# Extract features
extractor = URLFeatureExtractor()
complex_features = []
for url in complex_test_urls:
    try:
        feats = extractor.extract(url)
        complex_features.append(feats)
    except Exception as e:
        print(f"‚ö†Ô∏è  Error extracting {url}: {e}")

X_complex = pd.DataFrame(complex_features)
X_complex = X_complex.reindex(columns=X.columns, fill_value=0)

# Predict with both thresholds
y_proba_complex = final_model.predict_proba(X_complex)[:, 1]
y_pred_default_complex = (y_proba_complex >= 0.5).astype(int)
y_pred_optimal_complex = (y_proba_complex >= optimal_threshold).astype(int)

# Calculate false positive rates
fp_default_complex = np.sum(y_pred_default_complex == 1)
fp_optimal_complex = np.sum(y_pred_optimal_complex == 1)

print(f"\nüìä Results:")
print(f"   False Positives (default 0.5):  {fp_default_complex}/{len(complex_test_urls)} ({fp_default_complex/len(complex_test_urls)*100:.1f}%)")
print(f"   False Positives (optimal {optimal_threshold:.2f}): {fp_optimal_complex}/{len(complex_test_urls)} ({fp_optimal_complex/len(complex_test_urls)*100:.1f}%)")
print(f"   Improvement: {fp_default_complex - fp_optimal_complex} URLs now correctly classified as safe")

# Detailed breakdown
print(f"\nüìã Detailed Analysis:")
print(f"{'URL':<60} {'Prob':<8} {'Default':<10} {'Optimal':<10}")
print("-"*90)

for i, url in enumerate(complex_test_urls):
    prob = y_proba_complex[i]
    pred_def = "‚ö†Ô∏è PHISH" if y_pred_default_complex[i] == 1 else "‚úÖ Safe"
    pred_opt = "‚ö†Ô∏è PHISH" if y_pred_optimal_complex[i] == 1 else "‚úÖ Safe"
    
    # Truncate URL for display
    url_short = url[:57] + "..." if len(url) > 60 else url
    
    print(f"{url_short:<60} {prob*100:>6.1f}% {pred_def:<10} {pred_opt:<10}")

# Save validation report
validation_report = {
    "test_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    "total_complex_urls": len(complex_test_urls),
    "false_positives_default": int(fp_default_complex),
    "false_positives_optimal": int(fp_optimal_complex),
    "false_positive_rate_default": float(fp_default_complex / len(complex_test_urls)),
    "false_positive_rate_optimal": float(fp_optimal_complex / len(complex_test_urls)),
    "improvement": int(fp_default_complex - fp_optimal_complex),
    "threshold_used": float(optimal_threshold),
    "detailed_results": [
        {
            "url": url,
            "probability": float(y_proba_complex[i]),
            "predicted_default": int(y_pred_default_complex[i]),
            "predicted_optimal": int(y_pred_optimal_complex[i])
        }
        for i, url in enumerate(complex_test_urls)
    ]
}

validation_json_path = "Models/2025/reports/false_positive_validation.json"
with open(validation_json_path, 'w') as f:
    json.dump(validation_report, f, indent=4)
print(f"\n‚úÖ Saved: false_positive_validation.json")

# ---------------------------------------------------
# STEP 8: Save model
# ---------------------------------------------------
print("\nüíæ Saving model files...")
joblib.dump(final_model, "Models/2025/model_2025.pkl")
joblib.dump(list(X.columns), "Models/2025/features_2025.pkl")
joblib.dump(training_summary, "Models/2025/model_2025_metadata.pkl")

print("\n‚úÖ All files saved:")
print("   - model_2025.pkl (model)")
print("   - features_2025.pkl (feature names)")
print("   - model_2025_metadata.pkl (metadata)")
print("   - reports/training_summary_latest2025.json")
print("   - reports/feature_importance_latest2025.csv")
print("   - reports/training_report_latest2025.txt")
print("   - reports/test_predictions_latest2025.csv")
print("   - reports/false_positive_validation.json")

print("\n" + "="*70)
print(" ‚úÖ TRAINING COMPLETE!")
print("="*70)
print(f"\nüìñ Key Files:")
print(f"   ‚Ä¢ Training report: Models/2025/reports/training_report_latest2025.txt")
print(f"   ‚Ä¢ Validation results: Models/2025/reports/false_positive_validation.json")
print(f"\nüí° Optimal Threshold: {optimal_threshold:.4f}")
print(f"   Use this threshold in production to reduce false positives!")
print("\n" + "="*70)